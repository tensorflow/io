{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kafka.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Robust machine learning on streaming data using Kafka and Tensorflow-IO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/kafka\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9wRVaOQZWgRc"
      },
      "source": [
        "Caution: In addition to python packages this notebook uses `sudo apt-get install` to install third party packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial focuses on streaming data from a [Kafka](https://docs.confluent.io/current/getting-started.html) cluster into a `tf.data.Dataset` which is then used in conjunction with `tf.keras` for training and inference.\n",
        "\n",
        "Kafka is primarily a distributed event-streaming platform which provides scalable and fault-tolerant streaming data across data pipelines. It is an essential technical component of a plethora of major enterprises where mission-critical data delivery is a primary requirement.\n",
        "\n",
        "**NOTE:** A basic understanding of the [kafka components](https://docs.confluent.io/current/kafka/introduction.html) will help you in following the tutorial with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup and usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required tensorflow-io and kafka packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48B9eAMMhAgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "2e823314-406b-4225-8db4-0ac3f20edcae"
      },
      "source": [
        "print(\"Installing the tensorflow-io package !\")\n",
        "!pip install tensorflow-io\n",
        "\n",
        "print(\"Installing the kafka-python package !\")\n",
        "!pip install kafka-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing the tensorflow-io package !\n",
            "Collecting tensorflow-io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/5a/7a11179b0376df1fbac5f0af46819c0f990324fa3ee90eeb3110f683b129/tensorflow_io-0.15.0-cp36-cp36m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-io) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.9.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.12.4)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.18.5)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (49.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io) (3.1.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.15.0\n",
            "Installing the kafka-python package !\n",
            "Collecting kafka-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/34/12f219f7f9e68e79a54874d26fbe974db1ab4efac4e6dae665b421df48f9/kafka_python-2.0.1-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m6KXZuTBWgRm",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgO11GTJaTj",
        "colab_type": "text"
      },
      "source": [
        "### Validate tf and tfio imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX74RKfZ_TdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3467f8e1-5c87-4c16-eeca-0e6b938ffefe"
      },
      "source": [
        "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow-io version: 0.15.0\n",
            "tensorflow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "### Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUj0878jPyz7",
        "colab": {}
      },
      "source": [
        "!curl -sSOL http://packages.confluent.io/archive/5.4/confluent-community-5.4.1-2.12.tar.gz\n",
        "!tar -xzf confluent-community-5.4.1-2.12.tar.gz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F",
        "colab_type": "text"
      },
      "source": [
        "We use the default configurations for spinning up these instances as provided by the confluent package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9ujlunrWgRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "803b2cce-5010-46bf-edee-b324611d9a33"
      },
      "source": [
        "!cd confluent-5.4.1 && bin/zookeeper-server-start -daemon etc/kafka/zookeeper.properties\n",
        "!cd confluent-5.4.1 && bin/kafka-server-start -daemon etc/kafka/server.properties\n",
        "!cd confluent-5.4.1 && bin/schema-registry-start -daemon etc/schema-registry/schema-registry.properties\n",
        "!echo \"Waiting for 10 secs until kafka, zookeeper and schema registry services are up and running\"\n",
        "!sleep 10\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting for 10 secs until kafka, zookeeper and schema registry services are up and running\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD",
        "colab_type": "text"
      },
      "source": [
        "Once the instances are started as daemon processes, we can grep for `kafka` in the processes list. The three java processes correspond to zookeeper, kafka and the schema-registry instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LqMJ1BEHm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "892534ba-14a0-45c3-b746-f508dbb176a1"
      },
      "source": [
        "!ps -ef | grep kafka"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root         180       1 13 16:52 ?        00:00:02 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Xlog:gc*:file=/content/confluent-5.4.1/bin/../logs/zookeeper-gc.log:time,tags:filecount=10,filesize=102400 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/confluent-5.4.1/bin/../logs -Dlog4j.configuration=file:bin/../etc/kafka/log4j.properties -cp /content/confluent-5.4.1/bin/../share/java/kafka/*:/content/confluent-5.4.1/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/content/confluent-5.4.1/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/* org.apache.zookeeper.server.quorum.QuorumPeerMain etc/kafka/zookeeper.properties\n",
            "root         226       1 59 16:52 ?        00:00:09 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Xlog:gc*:file=/content/confluent-5.4.1/bin/../logs/kafkaServer-gc.log:time,tags:filecount=10,filesize=102400 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/confluent-5.4.1/bin/../logs -Dlog4j.configuration=file:bin/../etc/kafka/log4j.properties -cp /content/confluent-5.4.1/bin/../share/java/kafka/*:/content/confluent-5.4.1/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/content/confluent-5.4.1/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/* io.confluent.support.metrics.SupportedKafka etc/kafka/server.properties\n",
            "root         349     101  0 16:52 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh",
        "colab_type": "text"
      },
      "source": [
        "Create the kafka topics with the following specs:\n",
        "\n",
        "- susy-train: partitions=1, replication-factor=1 \n",
        "- susy-test: partitions=2, replication-factor=1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXJWqMmWnPyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b85f1c10-0c24-4ee3-9643-d10cf2b6b42c"
      },
      "source": [
        "!confluent-5.4.1/bin/kafka-topics --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 1 --topic susy-train\n",
        "!confluent-5.4.1/bin/kafka-topics --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 2 --topic susy-test\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created topic susy-train.\n",
            "Created topic susy-test.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC",
        "colab_type": "text"
      },
      "source": [
        "Describe the topic for details on the configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apCf9pfVnwn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "75265be4-35ab-49b4-a215-2885ab9386c8"
      },
      "source": [
        "!confluent-5.4.1/bin/kafka-topics --bootstrap-server 127.0.0.1:9092 --describe --topic susy-train\n",
        "!confluent-5.4.1/bin/kafka-topics --bootstrap-server 127.0.0.1:9092 --describe --topic susy-test\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: susy-train\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-train\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "Topic: susy-test\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-test\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "\tTopic: susy-test\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVnz3Pjot9t",
        "colab_type": "text"
      },
      "source": [
        "The replication factor 1 indicates that the data is not being replicated. This is due to the presence of a single broker in our kafka setup.\n",
        "\n",
        "**NOTE:** In production systems, the number of bootstrap servers can be in the range of 100's of nodes. That is where the fault-tolerance using replication comes into picture.\n",
        "\n",
        "Please refer the [docs](https://kafka.apache.org/documentation/#replication) for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCy3zaCQJ7-",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "**NOTE:** In actual scenarios, since kafka is an event streaming platform, data from various sources can be written into kafka. For instance:\n",
        "\n",
        "- syslog records\n",
        "- Web traffic logs\n",
        "- Firewall logs\n",
        "- Astronomical entity metrics\n",
        "- IoT sensor data\n",
        "- Product reviews\n",
        "- Fashion images and many more.\n",
        "\n",
        "#### Description\n",
        "\n",
        "For the purpose of this tutorial, we will download the [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY#) dataset and feed it into kafka manually, so as to simulate a streaming environment.\n",
        "\n",
        "Th goal of this classification problem is to distinguish between a signal process which produces supersymmetric particles and a background process which does not. In our case, we simulate a streaming enviornment where we train a model to classify these events and then infer on the test data by utilizing tensorflow-io.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emslB2EGQMCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -sSOL https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n",
        "!gzip -d SUSY.csv.gz"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7",
        "colab_type": "text"
      },
      "source": [
        "#### Explore the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18aR_MsOKToc",
        "colab_type": "text"
      },
      "source": [
        "The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features).\n",
        "\n",
        "**NOTE:** The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXyocIdKRSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLUMNS = [\n",
        "          #  labels\n",
        "           'class',\n",
        "          #  low-level features\n",
        "           'lepton_1_pT',\n",
        "           'lepton_1_eta',\n",
        "           'lepton_1_phi',\n",
        "           'lepton_2_pT',\n",
        "           'lepton_2_eta',\n",
        "           'lepton_2_phi',\n",
        "           'missing_energy_magnitude',\n",
        "           'missing_energy_phi',\n",
        "          #  high-level derived features\n",
        "           'MET_rel',\n",
        "           'axial_MET',\n",
        "           'M_R',\n",
        "           'M_TR_2',\n",
        "           'R',\n",
        "           'MT2',\n",
        "           'S_R',\n",
        "           'M_Delta_R',\n",
        "           'dPhi_r_b',\n",
        "           'cos(theta_r1)'\n",
        "           ]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-yt_c9u0sH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "69d2adef-4fba-4489-ce11-d52688f625e7"
      },
      "source": [
        "susy_df = pd.read_csv('SUSY.csv', header=None)\n",
        "susy_df.columns=COLUMNS\n",
        "susy_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>lepton_1_pT</th>\n",
              "      <th>lepton_1_eta</th>\n",
              "      <th>lepton_1_phi</th>\n",
              "      <th>lepton_2_pT</th>\n",
              "      <th>lepton_2_eta</th>\n",
              "      <th>lepton_2_phi</th>\n",
              "      <th>missing_energy_magnitude</th>\n",
              "      <th>missing_energy_phi</th>\n",
              "      <th>MET_rel</th>\n",
              "      <th>axial_MET</th>\n",
              "      <th>M_R</th>\n",
              "      <th>M_TR_2</th>\n",
              "      <th>R</th>\n",
              "      <th>MT2</th>\n",
              "      <th>S_R</th>\n",
              "      <th>M_Delta_R</th>\n",
              "      <th>dPhi_r_b</th>\n",
              "      <th>cos(theta_r1)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972861</td>\n",
              "      <td>0.653855</td>\n",
              "      <td>1.176225</td>\n",
              "      <td>1.157156</td>\n",
              "      <td>-1.739873</td>\n",
              "      <td>-0.874309</td>\n",
              "      <td>0.567765</td>\n",
              "      <td>-0.175000</td>\n",
              "      <td>0.810061</td>\n",
              "      <td>-0.252552</td>\n",
              "      <td>1.921887</td>\n",
              "      <td>0.889637</td>\n",
              "      <td>0.410772</td>\n",
              "      <td>1.145621</td>\n",
              "      <td>1.932632</td>\n",
              "      <td>0.994464</td>\n",
              "      <td>1.367815</td>\n",
              "      <td>0.040714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.667973</td>\n",
              "      <td>0.064191</td>\n",
              "      <td>-1.225171</td>\n",
              "      <td>0.506102</td>\n",
              "      <td>-0.338939</td>\n",
              "      <td>1.672543</td>\n",
              "      <td>3.475464</td>\n",
              "      <td>-1.219136</td>\n",
              "      <td>0.012955</td>\n",
              "      <td>3.775174</td>\n",
              "      <td>1.045977</td>\n",
              "      <td>0.568051</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448410</td>\n",
              "      <td>0.205356</td>\n",
              "      <td>1.321893</td>\n",
              "      <td>0.377584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444840</td>\n",
              "      <td>-0.134298</td>\n",
              "      <td>-0.709972</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>-1.613871</td>\n",
              "      <td>-0.768661</td>\n",
              "      <td>1.219918</td>\n",
              "      <td>0.504026</td>\n",
              "      <td>1.831248</td>\n",
              "      <td>-0.431385</td>\n",
              "      <td>0.526283</td>\n",
              "      <td>0.941514</td>\n",
              "      <td>1.587535</td>\n",
              "      <td>2.024308</td>\n",
              "      <td>0.603498</td>\n",
              "      <td>1.562374</td>\n",
              "      <td>1.135454</td>\n",
              "      <td>0.180910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.381256</td>\n",
              "      <td>-0.976145</td>\n",
              "      <td>0.693152</td>\n",
              "      <td>0.448959</td>\n",
              "      <td>0.891753</td>\n",
              "      <td>-0.677328</td>\n",
              "      <td>2.033060</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>3.046260</td>\n",
              "      <td>-1.005285</td>\n",
              "      <td>0.569386</td>\n",
              "      <td>1.015211</td>\n",
              "      <td>1.582217</td>\n",
              "      <td>1.551914</td>\n",
              "      <td>0.761215</td>\n",
              "      <td>1.715464</td>\n",
              "      <td>1.492257</td>\n",
              "      <td>0.090719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.309996</td>\n",
              "      <td>-0.690089</td>\n",
              "      <td>-0.676259</td>\n",
              "      <td>1.589283</td>\n",
              "      <td>-0.693326</td>\n",
              "      <td>0.622907</td>\n",
              "      <td>1.087562</td>\n",
              "      <td>-0.381742</td>\n",
              "      <td>0.589204</td>\n",
              "      <td>1.365479</td>\n",
              "      <td>1.179295</td>\n",
              "      <td>0.968218</td>\n",
              "      <td>0.728563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.083158</td>\n",
              "      <td>0.043429</td>\n",
              "      <td>1.154854</td>\n",
              "      <td>0.094859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  lepton_1_pT  lepton_1_eta  ...  M_Delta_R  dPhi_r_b  cos(theta_r1)\n",
              "0    0.0     0.972861      0.653855  ...   0.994464  1.367815       0.040714\n",
              "1    1.0     1.667973      0.064191  ...   0.205356  1.321893       0.377584\n",
              "2    1.0     0.444840     -0.134298  ...   1.562374  1.135454       0.180910\n",
              "3    1.0     0.381256     -0.976145  ...   1.715464  1.492257       0.090719\n",
              "4    1.0     1.309996     -0.690089  ...   0.043429  1.154854       0.094859\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlNuW7xbu6o8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "051ef3c2-a16c-4433-d2c2-64b9a4103c8c"
      },
      "source": [
        "# total number of datapoints and columns\n",
        "len(susy_df), len(susy_df.columns)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000000, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d5hLMz81LHu",
        "colab_type": "text"
      },
      "source": [
        "**Since the number of records are huge, let's take only a fraction of the dataset so that we spend less time on the moving the data and spend more time on understanding the functionality of the api.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Cg22bU0-na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42daf3c5-ddf2-4fc8-f10c-fff1547b1542"
      },
      "source": [
        "df=susy_df.sample(frac=0.02, replace=True, random_state=1)\n",
        "len(df), len(df[df[\"class\"]==0]), len(df[df[\"class\"]==1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 54175, 45825)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P",
        "colab_type": "text"
      },
      "source": [
        "#### Split the dataset\n",
        "\n",
        "We have a pretty balanced dataset for the purpose of this tutorial. We will now split the dataset into training and testing sets and store them in kafka. Thus, simulating an environment of continuous remote data retrieval for training and inference purposes.\n",
        "\n",
        "> **NOTE:** Please do not confuse the training step with online training. It's an entirely different paradigm of training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ku_X0Wld59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dc7f1dbc-46a6-4b2c-95f2-a7c2f982e939"
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"class\"], axis=1)\n",
        "y_train_df = train_df[\"class\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"class\"], axis=1)\n",
        "y_test_df = test_df[\"class\"]\n",
        "\n",
        "# We are splitting the data here so as to set the kafka message keys based on labels\n",
        "# This helps us in storing data in multiple-partitions and retrieve the data efficiently\n",
        "# using the consumer groups\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  60000\n",
            "Number of testing sample:  40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHXk0x2MXVgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06a5e692-fb6f-4750-cf67-c8285df7892e"
      },
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 60000, 40000, 40000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL",
        "colab_type": "text"
      },
      "source": [
        "> #### Store the train and test data in kafka\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwFImSqncLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed91d092-a8c8-455e-ee23-e5c83c3c84ca"
      },
      "source": [
        "\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"susy-test\", zip(x_test, y_test))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrote 60000 messages into topic: susy-train\n",
            "Wrote 40000 messages into topic: susy-test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58q52py93jEf",
        "colab_type": "text"
      },
      "source": [
        "#### Define the tfio train dataset\n",
        "\n",
        "We will be utilizing the `IODataset` class for streaming data from kafka into tensorflow. The class inherits from `tf.data.Dataset` and thus has all the useful functionalities of `tf.data.Dataset` out of the box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHOcitbW2_d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_kafka_item(item):\n",
        "  message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(item.key)\n",
        "  return (message, key)\n",
        "\n",
        "BATCH_SIZE=64\n",
        "SHUFFLE_BUFFER_SIZE=64\n",
        "train_ds = tfio.IODataset.from_kafka('susy-train', partition=0, offset=0)\n",
        "train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "train_ds = train_ds.map(decode_kafka_item)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x84lZJY164RI",
        "colab_type": "text"
      },
      "source": [
        "### Build and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHtpAMqLqmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the parameters\n",
        "\n",
        "OPTIMIZER=\"adam\"\n",
        "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "METRICS=['accuracy']\n",
        "EPOCHS=20\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBmxxuj63jZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e3c7ef34-400c-4c06-d22b-66aff8a358d1"
      },
      "source": [
        "# design/build the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(NUM_COLUMNS,)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               2432      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 68,481\n",
            "Trainable params: 68,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTDFVxpSLfXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJMg-saLgeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "205122ef-0c5c-44a0-d7f6-23eb2c4cf7cc"
      },
      "source": [
        "# fit the model\n",
        "model.fit(train_ds, epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.6220 - accuracy: 0.7517\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.6141 - accuracy: 0.7731\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6120 - accuracy: 0.7762\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6109 - accuracy: 0.7806\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6106 - accuracy: 0.7820\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6099 - accuracy: 0.7821\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.6096 - accuracy: 0.7834\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6094 - accuracy: 0.7844\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6093 - accuracy: 0.7832\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6090 - accuracy: 0.7849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f06ac848d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPy0Ka21QII5",
        "colab_type": "text"
      },
      "source": [
        "**NOTE:** Since we have used only a fraction of the dataset, our accuracy is limited to ~78% during the training phase. However, please feel free to store additional data in kafka for better a model performance. Also, since our goal was to just demonstrate the functionality of the tfio kafka datasets we have used a smaller and less-complicated neural network. However, one can increase the complexity of the model, modify the learning strategy, tune hyper-parameters etc for exploration purposes. For a baseline approach, please refer to this [article](https://www.nature.com/articles/ncomms5308#Sec11)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJW8za2qm4c",
        "colab_type": "text"
      },
      "source": [
        "### Infer on the test data\n",
        "\n",
        "For faster and scalable inference, we utilize the\n",
        "`KafkaGroupIODataset` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjnM81lPROen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "429bdae4-0556-4236-b521-6bfded052319"
      },
      "source": [
        "# dataset to stream for the susy-test topic\n",
        "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-test\"],\n",
        "    group_id=\"testcg\",\n",
        "    servers=\"127.0.0.1:9092\",\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(decode_kafka_item)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)\n",
        "\n",
        "# evaluate the performance of the model on the test data\n",
        "res = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", res)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 18s 29ms/step - loss: 0.6090 - accuracy: 0.7874\n",
            "test loss, test acc: [0.6090106964111328, 0.7874000072479248]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8j3bZsSF6u",
        "colab_type": "text"
      },
      "source": [
        "> **NOTE:** Though this class can be used for training purposes, there are caveats which need to be addressed. Once all the messages are read from kafka and the latest offsets are committed using the `KafkaGroupIODataset`, the consumer doesn't restart reading the messages from the beginning. Thus, while training, it is possible only to train for a single epoch with the data continuously flowing in.\n",
        "\n",
        "This kind of a functionality has limited use cases during the training phase wherein, once a datapoint has been consumed by the model it is no longer required and can be discarded.\n",
        "\n",
        "**However, this functionality shines when it comes to robust inference with exactly-once semantics.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Chcbd9xThl",
        "colab_type": "text"
      },
      "source": [
        "#### Track the offset lag of the `testcg` consumer group w.r.t `susy-test` topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uz3km0RxUG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c39dbe3d-4382-4013-9d19-4e80f7712e0a"
      },
      "source": [
        "!confluent-5.4.1/bin/kafka-consumer-groups --bootstrap-server 127.0.0.1:9092 --describe --group testcg\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                  HOST            CLIENT-ID\n",
            "testcg          susy-test       0          21833           21833           0               rdkafka-0556cae8-c449-48af-9c2b-77224c3919b5 /172.28.0.2     rdkafka\n",
            "testcg          susy-test       1          18167           18167           0               rdkafka-0556cae8-c449-48af-9c2b-77224c3919b5 /172.28.0.2     rdkafka\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Wg0_eXMKL9",
        "colab_type": "text"
      },
      "source": [
        "Once the `current-offset` matches the `log-end-offset` for all the partitions, it indicates that the consumers have fetched all the messages from the kafka topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QAS_3k1y3u",
        "colab_type": "text"
      },
      "source": [
        "### References:\n",
        "\n",
        "- Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014)\n",
        "\n",
        "- SUSY Dataset: https://archive.ics.uci.edu/ml/datasets/SUSY#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FONyXboRYe51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}