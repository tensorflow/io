{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c774b6fdf4b1"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9e30da",
      "metadata": {
        "id": "7857033a12ad"
      },
      "source": [
        "# DAOS Filesystem with Tensorflow (Using MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41881aed035d"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/daos\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/daos.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/daos.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/daos.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b37505",
      "metadata": {
        "id": "e5708f933d28"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to use read and write files on [DAOS Filesystem](https://docs.daos.io/) with TensorFlow, through TensorFlow IO's DAOS file system integration.\n",
        "\n",
        "A machine running DAOS natively or through a [docker emulator](https://github.com/daos-stack/daos/tree/master/utils/docker) is needed to run this tutorial and/or use the Tensorflow IO DAOS integration. The DAOS Pool and Container used for this tutorial will be created and deleted within this tutorial, where you will be training and testing a simple Neural Network on the MNIST Dataset loaded from the DAOS File System Plugin.\n",
        "\n",
        "The pool and container id or label are part of the filename uri:\n",
        "```\n",
        "daos://<pool_id>/<cont_id>/<path>\n",
        "daos://<pool-label>/cont-label/<path>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3ef6a9",
      "metadata": {
        "id": "b27be7087d0e"
      },
      "source": [
        "## Setup and usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad41a1a",
      "metadata": {
        "id": "e20c1d316af6"
      },
      "source": [
        "### Install required packages, and restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e35916b",
      "metadata": {
        "id": "5de1951509cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow-io in /home/omar/.local/lib/python3.8/site-packages (0.20.0)\n",
            "Requirement already satisfied: tensorflow<2.7.0,>=2.6.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.20.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow-io) (0.20.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.39.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.37.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.13.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: clang~=5.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (45.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.22.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/omar/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/omar/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/omar/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/omar/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/omar/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/omar/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x \n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "!pip install tensorflow-io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7de300",
      "metadata": {
        "id": "d5e736c41c99"
      },
      "source": [
        "### Create Pool and Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "79528fed",
      "metadata": {
        "id": "fb83b02da201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/sh: 1: dmg: not found\n",
            "/usr/bin/sh: 1: daos: not found\n"
          ]
        }
      ],
      "source": [
        "!dmg -i pool create -s 500M TEST_POOL\n",
        "!daos cont create --pool=TEST_POOL --type=POSIX TEST_CONT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9e03445ca2b"
      },
      "source": [
        "Importing the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d707f548ed"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5707958e9b2"
      },
      "source": [
        "Initializing our dfs path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b3d02b4bdce"
      },
      "outputs": [],
      "source": [
        "dfs_url = \"daos://TEST_POOL/TEST_CONT/\" # This the path you'll be using to load and access the dataset\n",
        "pwd = !pwd\n",
        "posix_url = pwd[0] + \"/tests/test_dfs/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb041488c5cc"
      },
      "source": [
        "Install Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d67936c99b0"
      },
      "outputs": [],
      "source": [
        "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/train.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/train_labels.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/test.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/test_labels.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40e9a30808c"
      },
      "source": [
        "Copying the Data from the POSIX Filesystem to the DAOS Filesystem under the pool and container you just created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7a9cb50149f"
      },
      "outputs": [],
      "source": [
        "file_names = [\"train.gz\", \"test.gz\", \"train_labels.gz\", \"test_labels.gz\"]\n",
        "for file in file_names:\n",
        "  tf.io.gfile.copy(posix_url + file, dfs_url + file, True)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1879a8438874"
      },
      "source": [
        "Checking Our Training Images and Training Labels Exist under the specified pool and container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4831a44b46c4"
      },
      "outputs": [],
      "source": [
        "images = dfs_url + \"train.gz\"\n",
        "labels = dfs_url + \"train_labels.gz\"\n",
        "if tf.io.gfile.exists(images) and tf.io.gfile.exists(labels):\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7453b98fc59"
      },
      "source": [
        "Loading MNIST Data from the DFS using tensorflow-io's built in MNIST loading functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b904e495cf5"
      },
      "outputs": [],
      "source": [
        "d_train = tfio.IODataset.from_mnist(\n",
        "    images,\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43e0588f94e4"
      },
      "source": [
        "Pre-processing and Building a simple Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "250109055744"
      },
      "outputs": [],
      "source": [
        "# Shuffle the elements of the dataset.\n",
        "d_train = d_train.shuffle(buffer_size=1024)\n",
        "\n",
        "# By default image data is uint8, so convert to float32 using map().\n",
        "d_train = d_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
        "\n",
        "# prepare batches the data just like any other tf.data.Dataset\n",
        "d_train = d_train.batch(32)\n",
        "\n",
        "# Build the model.\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4129e8e2c1b4"
      },
      "source": [
        "Compiling the model you just built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c9302dea1da"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6193df954a7c"
      },
      "source": [
        "And finally, training on the dataset for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "629b3388da02"
      },
      "outputs": [],
      "source": [
        "history = model.fit(d_train, epochs=15, steps_per_epoch=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb6caec04ac"
      },
      "source": [
        "Plot of Loss vs Epoch during Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14d751d25850"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdcea14afec8"
      },
      "source": [
        "Check Test Data is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c235960fd75"
      },
      "outputs": [],
      "source": [
        "test_images = dfs_url + \"test.gz\"\n",
        "test_labels = dfs_url + \"test_labels.gz\"\n",
        "if tf.io.gfile.exists(test_images) and tf.io.gfile.exists(test_labels):\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bcaff8a9330"
      },
      "source": [
        "Apply same pre-processing and batching on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecd03ced1c6a"
      },
      "outputs": [],
      "source": [
        "d_test = tfio.IODataset.from_mnist(\n",
        "    test_images,\n",
        "    test_labels,\n",
        ")\n",
        "\n",
        "# Shuffle the elements of the dataset.\n",
        "d_test = d_test.shuffle(buffer_size=1024)\n",
        "\n",
        "# By default image data is uint8, so convert to float32 using map().\n",
        "d_test = d_test.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
        "\n",
        "# prepare batches the data just like any other tf.data.Dataset\n",
        "d_test = d_test.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4684eb989dd"
      },
      "source": [
        "Evaluate our model on both test and train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de7d13fa71ef"
      },
      "outputs": [],
      "source": [
        "_, train_acc = model.evaluate(d_train, verbose=0)\n",
        "_, test_acc = model.evaluate(d_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a006d7d2fcb"
      },
      "source": [
        "Prediction Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "701b75aea6c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "iterator = iter(d_test)\n",
        "elem = iterator.get_next()[0][0]\n",
        "plt.imshow(elem)\n",
        "prediction = model.predict(np.array([elem]))\n",
        "result = np.where(prediction[0] == np.amax(prediction[0]))\n",
        "print(\"Predicted Value is\" ,result[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21b12f92e0a2"
      },
      "source": [
        "### Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d847aaa57005"
      },
      "outputs": [],
      "source": [
        "!dmg -i pool destroy -f TEST_POOL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "daos.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
